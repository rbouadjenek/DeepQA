{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "#  Copyright (c) 2021. Mohamed Reda Bouadjenek, Deakin University              +\n",
    "#           Email:  reda.bouadjenek@deakin.edu.au                              +\n",
    "#                                                                              +\n",
    "#  Licensed under the Apache License, Version 2.0 (the \"License\");             +\n",
    "#   you may not use this file except in compliance with the License.           +\n",
    "#    You may obtain a copy of the License at:                                  +\n",
    "#                                                                              +\n",
    "#                 http://www.apache.org/licenses/LICENSE-2.0                   +\n",
    "#                                                                              +\n",
    "#    Unless required by applicable law or agreed to in writing, software       +\n",
    "#    distributed under the License is distributed on an \"AS IS\" BASIS,         +\n",
    "#    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  +\n",
    "#    See the License for the specific language governing permissions and       +\n",
    "#    limitations under the License.                                            +\n",
    "#                                                                              +\n",
    "# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To be run on Google Colab!\n",
    "# !mkdir preprocessing/\n",
    "# !wget --directory-prefix=preprocessing/  https://raw.githubusercontent.com/rbouadjenek/DeepQA/master/DeepQA/preprocessing/__init__.py   > /dev/null 2> /dev/null \n",
    "# !wget --directory-prefix=preprocessing/ https://raw.githubusercontent.com/rbouadjenek/DeepQA/master/DeepQA/preprocessing/preprocessing.py  > /dev/null 2> /dev/null \n",
    "# !pip install tokenizers transformers keras_metrics > /dev/null 2> /dev/null \n",
    "\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "use_tpu = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import json\n",
    "import tarfile\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.python.keras import backend as K\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "from transformers import BertTokenizer, BertConfig, TFBertModel\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from preprocessing.preprocessing import create_squad_examples, create_inputs_targets, get_SQuAD1, get_SQuAD2, get_NewsQA\n",
    "from keras.layers import LSTM, Input\n",
    "# from keras.engine import InputSpec\n",
    "from tensorflow.keras.layers import InputSpec\n",
    "# from keras.engine.input_spec import InputSpec\n",
    "from keras.activations import tanh, softmax\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "# Set the random seeds\n",
    "np.random.seed(1)\n",
    "random.seed(1)\n",
    "tf.random.set_seed(1)\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext tensorboard\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set max length value\n",
    "max_len = 300\n",
    "max_ans_len = 10\n",
    "# Create the tokenizer\n",
    "save_path = os.path.expanduser(\"~\") + \"/.bert_base_uncased/\"\n",
    "if not os.path.exists(save_path):\n",
    "    slow_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "    os.makedirs(save_path)\n",
    "    slow_tokenizer.save_pretrained(save_path)\n",
    "tokenizer = BertWordPieceTokenizer(save_path + \"vocab.txt\", lowercase=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the raw data\n",
    "\n",
    "# raw_train_data, raw_val_data, raw_test_data = get_SQuAD1()\n",
    "raw_train_data, raw_val_data, raw_test_data = get_SQuAD2()\n",
    "# raw_train_data, raw_val_data, raw_test_data = get_NewsQA()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35/35 [00:08<00:00,  4.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size:  11792\n",
      "skipped_questions:  6\n",
      "skipped_questions:  81\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/reda/.local/lib/python3.8/site-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    }
   ],
   "source": [
    "# Prepare the data\n",
    "\n",
    "# train_squad_examples, train_skipped_questions = create_squad_examples(raw_train_data, max_len, tokenizer, False)\n",
    "# x_train, y1_train, y2_train, y3_train, y4_train, train_total_skipped_questions = create_inputs_targets(train_squad_examples)\n",
    "# print('Size: ', len(y1_train[0]))\n",
    "# print('skipped_questions: ', train_skipped_questions)\n",
    "# print('skipped_questions: ', train_total_skipped_questions)\n",
    "\n",
    "# eval_squad_examples, val_skipped_questions = create_squad_examples(raw_val_data, max_len, tokenizer)\n",
    "# x_eval, y1_eval, y2_eval, y3_eval, y4_eval, val_total_skipped_questions = create_inputs_targets(eval_squad_examples)\n",
    "# print('Size: ', len(y1_eval[0]))\n",
    "# print('skipped_questions: ', val_skipped_questions)\n",
    "# print('skipped_questions: ', val_total_skipped_questions)\n",
    "\n",
    "test_squad_examples, test_skipped_questions = create_squad_examples(raw_test_data, max_len, tokenizer)\n",
    "x_test, y1_test, y2_test, y3_test, y4_test, test_total_skipped_questions = create_inputs_targets(test_squad_examples)\n",
    "print('Size: ', len(y1_test[0]))\n",
    "print('skipped_questions: ', test_skipped_questions)\n",
    "print('skipped_questions: ', test_total_skipped_questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# Create the keras model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "#  Acknowledgement:                                                            +\n",
    "#          The code in this cell has been borrowed from keon                   +\n",
    "#                     https://github.com/keon                                  +\n",
    "#               https://github.com/keon/pointer-networks                       +\n",
    "#                                                                              +\n",
    "# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "\n",
    "class Attention(keras.layers.Layer):\n",
    "    \"\"\"\n",
    "        Attention layer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hidden_dimensions, name='attention'):\n",
    "        super(Attention, self).__init__(name=name, trainable=True)\n",
    "        self.W1 = keras.layers.Dense(hidden_dimensions, use_bias=False)\n",
    "        self.W2 = keras.layers.Dense(hidden_dimensions, use_bias=False)\n",
    "        self.V = keras.layers.Dense(1, use_bias=False)\n",
    "\n",
    "    def call(self, encoder_outputs, dec_output, mask=None):\n",
    "\n",
    "        w1_e = self.W1(encoder_outputs)\n",
    "        w2_d = self.W2(dec_output)\n",
    "        tanh_output = tanh(w1_e + w2_d)\n",
    "        v_dot_tanh = self.V(tanh_output)\n",
    "        if mask is not None:\n",
    "            v_dot_tanh += (mask * -1e9)\n",
    "        attention_weights = softmax(v_dot_tanh, axis=1)\n",
    "        att_shape = K.shape(attention_weights)\n",
    "        return K.reshape(attention_weights, (att_shape[0], att_shape[1]))\n",
    "\n",
    "class Decoder(keras.layers.Layer):\n",
    "    \"\"\"\n",
    "        Decoder class for PointerLayer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hidden_dimensions):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.lstm = keras.layers.LSTM(\n",
    "            hidden_dimensions, return_sequences=False, return_state=True)\n",
    "\n",
    "    def call(self, x, hidden_states):\n",
    "        dec_output, state_h, state_c = self.lstm(\n",
    "            x, initial_state=hidden_states)\n",
    "        return dec_output, [state_h, state_c]\n",
    "\n",
    "    def get_initial_state(self, inputs):\n",
    "        return self.lstm.get_initial_state(inputs)\n",
    "\n",
    "    def process_inputs(self, x_input, initial_states, constants):\n",
    "        return self.lstm._process_inputs(x_input, initial_states, constants)\n",
    "    \n",
    "\n",
    "class PointerLSTM(keras.layers.Layer):\n",
    "    \"\"\"\n",
    "        PointerLSTM\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hidden_dimensions, output_length=2, name='pointer', **kwargs):\n",
    "        super(PointerLSTM, self).__init__(\n",
    "            hidden_dimensions, name=name, **kwargs)\n",
    "        self.hidden_dimensions = hidden_dimensions\n",
    "        self.attention = Attention(hidden_dimensions)\n",
    "        self.decoder = Decoder(hidden_dimensions)\n",
    "        self.output_length = output_length\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super(PointerLSTM, self).build(input_shape)\n",
    "        self.input_spec = [InputSpec(shape=input_shape)]\n",
    "\n",
    "    def call(self, x, training=None, mask=None, states=None):\n",
    "        \"\"\"\n",
    "        :param Tensor x: Should be the output of the decoder\n",
    "        :param Tensor states: last state of the decoder\n",
    "        :param Tensor mask: The mask to apply\n",
    "        :return: Pointers probabilities\n",
    "        \"\"\"\n",
    "\n",
    "        input_shape = self.input_spec[0].shape\n",
    "        en_seq = x\n",
    "        x_input = x[:, input_shape[1] - 1, :]\n",
    "        x_input = K.repeat(x_input, input_shape[1])\n",
    "        if states:\n",
    "            initial_states = states\n",
    "        else:\n",
    "            initial_states = self.decoder.get_initial_state(x_input)\n",
    "\n",
    "        constants = []\n",
    "        preprocessed_input, _, constants = self.decoder.process_inputs(\n",
    "            x_input, initial_states, constants)\n",
    "        constants.append(en_seq)\n",
    "\n",
    "        \n",
    "        last_output, outputs, states = K.rnn(self.step, preprocessed_input[:,0:self.output_length,:],\n",
    "                                             initial_states,\n",
    "                                             go_backwards=self.decoder.lstm.go_backwards,\n",
    "                                             constants=constants,\n",
    "                                             input_length=input_shape[1])\n",
    "        \n",
    "        return outputs\n",
    "\n",
    "    def step(self, x_input, states):\n",
    "        x_input = K.expand_dims(x_input,1)\n",
    "        input_shape = self.input_spec[0].shape\n",
    "        en_seq = states[-1]\n",
    "        _, [h, c] = self.decoder(x_input, states[:-1])\n",
    "        dec_seq = K.repeat(h, input_shape[1])\n",
    "        probs = self.attention(dec_seq, en_seq)\n",
    "        return probs, [h, c]\n",
    "\n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        # output shape is not affected by the attention component\n",
    "        return (input_shape[0], input_shape[1], input_shape[1])\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[1], input_shape[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "def create_model(learn_rate=5e-5, dropout_prob=0.3):\n",
    "    ## BERT encoder \n",
    "    configuration = BertConfig(hidden_dropout_prob=dropout_prob, attention_probs_dropout_prob=dropout_prob) \n",
    "    encoder = TFBertModel.from_pretrained(\"bert-base-uncased\", config=configuration)\n",
    "    ## QA Model\n",
    "    input_ids = layers.Input(shape=(max_len,), dtype=tf.int32, name=\"input_ids\")\n",
    "    token_type_ids = layers.Input(shape=(max_len,), dtype=tf.int32, name=\"token_type_ids\")\n",
    "    attention_mask = layers.Input(shape=(max_len,), dtype=tf.int32, name=\"attention_mask\") \n",
    "    \n",
    "    embedding = encoder(input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask)[0]\n",
    "    \n",
    "#     state_h = embedding[:,0,:]\n",
    "#     state_c = embedding[:,0,:]\n",
    "#     print(state_h.shape)\n",
    "    \n",
    "    decoder = PointerLSTM(768, output_length=max_ans_len, name=\"decoder\")(embedding)\n",
    "    \n",
    "    model = keras.Model(\n",
    "        inputs=[input_ids, token_type_ids, attention_mask],\n",
    "        outputs=decoder,\n",
    "    )\n",
    "    loss = keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
    "\n",
    "    optimizer = keras.optimizers.Adam(lr=learn_rate)\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'],\n",
    "                  run_eagerly=False) \n",
    "    print(model.summary())\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_tpu:\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "    tf.config.experimental_connect_to_cluster(tpu)\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
    "\n",
    "for lr in [9e-06, 1e-05, 3e-05, 5e-05, 7e-05, 9e-05, 1e-04]:\n",
    "    for dropout_prob in [0, 0.2, 0.4, 0.6]:\n",
    "        print(\"Learning rate = \" + str(lr) + \", dropout_prob = \" + str(dropout_prob))\n",
    "        np.random.seed(1)\n",
    "        random.seed(1)\n",
    "        tf.random.set_seed(1)\n",
    "        if use_tpu: \n",
    "            with strategy.scope():\n",
    "                model = create_model(lr, dropout_prob)\n",
    "        else:\n",
    "            model = create_model(lr, dropout_prob) \n",
    "        history = model.fit(\n",
    "            x_train,\n",
    "            ,\n",
    "            validation_data=(x_eval,),\n",
    "            epochs=20, \n",
    "            verbose=1,\n",
    "            batch_size=128,\n",
    "            callbacks=[callback],\n",
    "        )\n",
    "        results = model.evaluate(x_eval, y2_eval, batch_size=128)\n",
    "        print(f\"Results validation: {results}, Learning rate =  {lr}, dropout_prob = {dropout_prob}\")\n",
    "        print('-'*50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the model with the best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters\n",
    "# SQuAD1.1\n",
    "# file_name = \"SQuAD1.1_\"\n",
    "# lr = 5e-05\n",
    "# dropout_prob = 0.2\n",
    "\n",
    "# SQuAD2.0\n",
    "# file_name = \"SQuAD2.0_\"\n",
    "# lr = 5e-05\n",
    "# dropout_prob = 0.2\n",
    "\n",
    "# NewsQA\n",
    "# file_name = \"NewsQA_\"\n",
    "# lr = 9E-06\n",
    "# dropout_prob = 0.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
    "np.random.seed(1)\n",
    "random.seed(1)\n",
    "tf.random.set_seed(1)\n",
    "if use_tpu: \n",
    "    with strategy.scope():\n",
    "        model = create_model(lr, dropout_prob)\n",
    "\n",
    "history = model.fit(\n",
    "    x_train,\n",
    "    ,\n",
    "    validation_data=(x_eval,),\n",
    "    epochs=20, \n",
    "    verbose=1,\n",
    "    batch_size=128,\n",
    "    callbacks=[callback],\n",
    ")\n",
    "model.evaluate(x_test, , batch_size=128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = model.predict(x_test, verbose=1)\n",
    "eval_pred = model.predict(x_eval, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zip and copy the results\n",
    "\n",
    "algorithm = \"Answer_Pointer_BM_\"\n",
    "file_list = []\n",
    "\n",
    "def save_to_file(name, data):\n",
    "    json_string = json.dumps(data.tolist())\n",
    "    file = algorithm + file_name + name\n",
    "    file_list.append(file)\n",
    "    with open(file, \"w\") as text_file:\n",
    "        text_file.write(json_string)\n",
    "\n",
    "save_to_file(\"test_pred.txt\", test_pred)\n",
    "save_to_file(\"eval_pred.txt\", eval_pred)\n",
    "\n",
    "save_to_file(\"y2_eval.txt\", y2_eval)\n",
    "save_to_file(\"y3_eval.txt\", np.array([v.tolist() for v in y3_eval]))\n",
    "save_to_file(\"y4_eval.txt\", y4_eval)\n",
    "\n",
    "save_to_file(\"y2_test.txt\", y2_test)\n",
    "save_to_file(\"y3_test.txt\", np.array([v.tolist() for v in y3_test]))\n",
    "save_to_file(\"y4_test.txt\", y4_test)\n",
    "\n",
    "\n",
    "\n",
    "save_to_file(\"token_type_eval.txt\", x_eval[1])\n",
    "save_to_file(\"token_type_test.txt\", x_test[1])\n",
    "\n",
    "save_to_file(\"input_ids_test.txt\", x_test[0])\n",
    "\n",
    "\n",
    "tar = tarfile.open(algorithm + file_name + \"predictions.tar.gz\", \"w:gz\")\n",
    "for name in file_list:\n",
    "    tar.add(name)\n",
    "tar.close()\n",
    "\n",
    "!mv *predictions.tar.gz /content/drive/MyDrive\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7fb610529040>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7fb610529040>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_ids (InputLayer)          [(None, 300)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "attention_mask (InputLayer)     [(None, 300)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "token_type_ids (InputLayer)     [(None, 300)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_bert_model (TFBertModel)     TFBaseModelOutputWit 109482240   input_ids[0][0]                  \n",
      "                                                                 attention_mask[0][0]             \n",
      "                                                                 token_type_ids[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "decoder (PointerLSTM)           (None, 10, None)     5902080     tf_bert_model[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 115,384,320\n",
      "Trainable params: 115,384,320\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = create_model(5e-05, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 4\n",
    "history = model.fit(\n",
    "        [x_test[0][0:i],x_test[1][0:i],x_test[2][0:i]],\n",
    "        y4_test[0:i],\n",
    "        epochs=20, \n",
    "        verbose=1,\n",
    "        batch_size=4,\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
