{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "#  Copyright (c) 2021. Mohamed Reda Bouadjenek, Deakin University              +\n",
    "#           Email:  reda.bouadjenek@deakin.edu.au                              +\n",
    "#                                                                              +\n",
    "#  Licensed under the Apache License, Version 2.0 (the \"License\");             +\n",
    "#   you may not use this file except in compliance with the License.           +\n",
    "#    You may obtain a copy of the License at:                                  +\n",
    "#                                                                              +\n",
    "#                 http://www.apache.org/licenses/LICENSE-2.0                   +\n",
    "#                                                                              +\n",
    "#    Unless required by applicable law or agreed to in writing, software       +\n",
    "#    distributed under the License is distributed on an \"AS IS\" BASIS,         +\n",
    "#    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  +\n",
    "#    See the License for the specific language governing permissions and       +\n",
    "#    limitations under the License.                                            +\n",
    "#                                                                              +\n",
    "# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To be run on Google Colab!\n",
    "# !mkdir preprocessing/\n",
    "# !wget --directory-prefix=preprocessing/  https://raw.githubusercontent.com/rbouadjenek/DeepQA/master/DeepQA/preprocessing/__init__.py   > /dev/null 2> /dev/null \n",
    "# !wget --directory-prefix=preprocessing/ https://raw.githubusercontent.com/rbouadjenek/DeepQA/master/DeepQA/preprocessing/preprocessing.py  > /dev/null 2> /dev/null \n",
    "# !pip install tokenizers transformers keras_metrics > /dev/null 2> /dev/null \n",
    "\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "use_tpu = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import json\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "from tensorflow.python.keras import backend as K\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "from transformers import BertTokenizer, BertConfig, TFBertModel\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from preprocessing.preprocessing import create_squad_examples, create_inputs_targets, get_SQuAD1, get_SQuAD2, get_NewsQA\n",
    "\n",
    "# Set the random seeds\n",
    "np.random.seed(1)\n",
    "random.seed(1)\n",
    "tf.random.set_seed(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set max length value\n",
    "max_len = 300\n",
    "# Create the tokenizer\n",
    "save_path = os.path.expanduser(\"~\") + \"/.bert_base_uncased/\"\n",
    "if not os.path.exists(save_path):\n",
    "    slow_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "    os.makedirs(save_path)\n",
    "    slow_tokenizer.save_pretrained(save_path)\n",
    "tokenizer = BertWordPieceTokenizer(save_path + \"vocab.txt\", lowercase=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the raw data\n",
    "\n",
    "# raw_train_data, raw_val_data, raw_test_data = get_SQuAD1()\n",
    "raw_train_data, raw_val_data, raw_test_data = get_SQuAD2()\n",
    "# raw_train_data, raw_val_data, raw_test_data = get_NewsQA()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Prepare the data\n",
    "\n",
    "train_squad_examples, train_skipped_questions = create_squad_examples(raw_train_data, max_len, tokenizer, False)\n",
    "x_train, y1_train, y2_train, y3_train, train_total_skipped_questions = create_inputs_targets(train_squad_examples)\n",
    "print('Size: ', len(y1_train[0]))\n",
    "print('skipped_questions: ', train_skipped_questions)\n",
    "print('skipped_questions: ', train_total_skipped_questions)\n",
    "\n",
    "eval_squad_examples, val_skipped_questions = create_squad_examples(raw_val_data, max_len, tokenizer)\n",
    "x_eval, y1_eval, y2_eval, y3_eval, val_total_skipped_questions = create_inputs_targets(eval_squad_examples)\n",
    "print('Size: ', len(y1_eval[0]))\n",
    "print('skipped_questions: ', val_skipped_questions)\n",
    "print('skipped_questions: ', val_total_skipped_questions)\n",
    "\n",
    "test_squad_examples, test_skipped_questions = create_squad_examples(raw_test_data, max_len, tokenizer)\n",
    "x_test, y1_test, y2_test, y3_test, test_total_skipped_questions = create_inputs_targets(test_squad_examples)\n",
    "print('Size: ', len(y1_test[0]))\n",
    "print('skipped_questions: ', test_skipped_questions)\n",
    "print('skipped_questions: ', test_total_skipped_questions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# Create the keras model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "def create_model(learn_rate=5e-5, dropout_prob=0.3):\n",
    "    ## BERT encoder \n",
    "    configuration = BertConfig(hidden_dropout_prob=dropout_prob, attention_probs_dropout_prob=dropout_prob) \n",
    "    encoder = TFBertModel.from_pretrained(\"bert-base-uncased\", config=configuration)\n",
    "    ## QA Model\n",
    "    input_ids = layers.Input(shape=(max_len,), dtype=tf.int32, name=\"input_ids\")\n",
    "    token_type_ids = layers.Input(shape=(max_len,), dtype=tf.int32, name=\"token_type_ids\")\n",
    "    attention_mask = layers.Input(shape=(max_len,), dtype=tf.int32, name=\"attention_mask\") \n",
    "    \n",
    "    embedding = encoder(input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask)[0]\n",
    "    \n",
    "    \n",
    "    start_logits = layers.Dense(1, name=\"start_logit\", use_bias=False)(embedding)    \n",
    "    start_logits = layers.Dropout(dropout_prob)(start_logits)\n",
    "    start_logits = layers.Flatten()(start_logits)\n",
    "\n",
    "    end_logits = layers.Dense(1, name=\"end_logit\", use_bias=False)(embedding)   \n",
    "    end_logits = layers.Dropout(dropout_prob)(end_logits)\n",
    "    end_logits = layers.Flatten()(end_logits)\n",
    "\n",
    "    start_probs = layers.Activation(keras.activations.softmax)(start_logits)\n",
    "    end_probs = layers.Activation(keras.activations.softmax)(end_logits)\n",
    "    \n",
    "    model = keras.Model(\n",
    "        inputs=[input_ids, token_type_ids, attention_mask],\n",
    "        outputs=[start_probs, end_probs],\n",
    "    )\n",
    "    loss = keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
    "    optimizer = keras.optimizers.Adam(lr=learn_rate)\n",
    "    model.compile(optimizer=optimizer, loss=[loss, loss], metrics=['accuracy'], \n",
    "                  run_eagerly=False)    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_tpu:\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "    tf.config.experimental_connect_to_cluster(tpu)\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
    "\n",
    "for lr in [9e-06, 1e-05, 3e-05, 5e-05, 7e-05, 9e-05, 1e-04]:\n",
    "    for dropout_prob in [0, 0.2]:\n",
    "        print(\"Learning rate = \" + str(lr) + \", dropout_prob = \" + str(dropout_prob))\n",
    "        np.random.seed(1)\n",
    "        random.seed(1)\n",
    "        tf.random.set_seed(1)\n",
    "        if use_tpu: \n",
    "            with strategy.scope():\n",
    "                model = create_model(lr, dropout_prob)\n",
    "        else:\n",
    "            model = create_model(lr, dropout_prob) \n",
    "        history = model.fit(\n",
    "            x_train,\n",
    "            y1_train,\n",
    "            validation_data=(x_eval,y1_eval),\n",
    "            epochs=20, \n",
    "            verbose=1,\n",
    "            batch_size=128,\n",
    "            callbacks=[callback],\n",
    "        )\n",
    "        results = model.evaluate(x_eval, y1_eval, batch_size=128)\n",
    "        print(f\"Results validation: {results}, Learning rate =  {lr}, dropout_prob = {dropout_prob}\")\n",
    "        print('-'*50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the model with the best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters\n",
    "# SQuAD2.0\n",
    "# file_name = \"SQuAD2.0_\"\n",
    "# lr = 5e-05\n",
    "# dropout_prob = 0.2\n",
    "\n",
    "# NewsQA\n",
    "# file_name = \"NewQA_\"\n",
    "# lr = 9E-06\n",
    "# dropout_prob = 0.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
    "\n",
    "np.random.seed(1)\n",
    "random.seed(1)\n",
    "tf.random.set_seed(1)\n",
    "if use_tpu: \n",
    "    with strategy.scope():\n",
    "        model = create_model(lr, dropout_prob)\n",
    "\n",
    "\n",
    "history = model.fit(\n",
    "            x_train,\n",
    "            y1_train,\n",
    "            validation_data=(x_eval,y1_eval),\n",
    "            epochs=20, \n",
    "            verbose=1,\n",
    "            batch_size=128,\n",
    "            callbacks=[callback],\n",
    "        )\n",
    "model.evaluate(x_test, y1_test, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred_start_baseline, test_pred_end_baseline = model.predict(x_test, verbose=1)\n",
    "eval_pred_start_baseline, eval_pred_end_baseline = model.predict(x_eval, verbose=1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithm = \"Baseline1_\"\n",
    "file_list = []\n",
    "\n",
    "def save_to_file(name, data):\n",
    "    json_string = json.dumps(data.tolist())\n",
    "    file = algorithm + file_name + name\n",
    "    file_list.append(file)\n",
    "    with open(file, \"w\") as text_file:\n",
    "        text_file.write(json_string)\n",
    "\n",
    "save_to_file(\"test_pred_start.txt\", test_pred_start_baseline)\n",
    "save_to_file(\"test_pred_end.txt\", test_pred_end_baseline)\n",
    "\n",
    "save_to_file(\"eval_pred_start.txt\", eval_pred_start_baseline)\n",
    "save_to_file(\"eval_pred_end.txt\", eval_pred_end_baseline)\n",
    "\n",
    "save_to_file(\"y2_eval.txt\",y2_eval)\n",
    "save_to_file(\"y3_eval.txt\", np.array([v.tolist() for v in y3_eval]))\n",
    "\n",
    "save_to_file(\"y2_test.txt\",y2_test)\n",
    "save_to_file(\"y3_test.txt\", np.array([v.tolist() for v in y3_test]))\n",
    "\n",
    "tar = tarfile.open(algorithm + file_name + \"predictions.tar.gz\", \"w:gz\")\n",
    "for name in file_list:\n",
    "    tar.add(name)\n",
    "tar.close()\n",
    "\n",
    "!mv *predictions.tar.gz /content/drive/MyDrive\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "random.seed(1)\n",
    "tf.random.set_seed(1)\n",
    "model = create_model(5e-05, 0) \n",
    "model.summary()\n",
    "i = 16\n",
    "history = model.fit(\n",
    "        [x_test[0][0:i],x_test[1][0:i],x_test[2][0:i]],\n",
    "        [y1_test[0][0:i],y1_test[1][0:i]],\n",
    "        epochs=2, \n",
    "        verbose=1,\n",
    "        batch_size=4,\n",
    "    )\n",
    "model.evaluate([x_test[0][0:i],x_test[1][0:i],x_test[2][0:i]],[y1_test[0][0:i],y1_test[1][0:i]], batch_size=4)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
